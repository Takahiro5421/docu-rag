# RAGシステムのアーキテクチャ

以下では社内ドキュメントQAシステムにおけるRetrieval-Augmented Generation (RAG) の基本的な処理フローと必要要件を整理します。

## 全体アーキテクチャ
図1はオフラインのインデックス構築フェーズとオンラインのクエリ応答フェーズから成るRAGパイプラインを示します。

- **インデックス構築フェーズ**: SharePoint から文書を収集し、テキスト変換・前処理後にチャンク分割を行います。各チャンクは埋め込みベクトルを計算してベクターデータベースへ保存します。
- **クエリ応答フェーズ**: ユーザー質問をベクトル化し、関連するチャンクを検索してLLMに文脈として与えます。LLMの生成結果に出典情報付与や整形処理を行い回答を返します。

### 文書インデックス構築
1. **SharePoint 取得**: Microsoft Graph API などでファイルをダウンロードし、増分クロールを行います。
2. **テキスト抽出**: Word、PowerPoint、Excel、PDF それぞれに適した方法でテキストを取得します。必要に応じOCRを使います。
3. **メタデータ付与**: タイトルや更新日時、アクセス権情報を記録します。
4. **前処理**: 不要な空白や改ページを除去し、見出し構造を保持したまま整形します。
5. **チャンク分割**: 見出しや段落単位を考慮して200〜300語程度でセマンティックチャンク化し、重なりも設けます。
6. **ベクトル変換**: 適切な多言語 / 日本語向け埋め込みモデルでベクトル化します。
7. **ベクターデータベース格納**: チャンクのベクトルとメタデータをChromaやFaiss等に保存します。必要に応じ全文検索エンジンにも登録します。
8. **更新監視**: 変更ログやWebhookで文書更新を検知し、再インデックスを行います。ストリーミング処理での即時アップサートも検討します。

## クエリ応答フロー
1. チャットUIから受け取った質問をベクトル化し、ベクターストアから類似チャンクを取得します。
2. 見つかったチャンクを文脈としてLLMに入力し、社内情報に基づく回答を生成します。
3. 回答には引用元などの出典情報を付与し、必要に応じてフォーマットを整えユーザーへ返します。

このアーキテクチャにより、社内の最新情報を活用したリアルタイムな質疑応答が可能になります。

## 高精度な情報検索戦略
検索段階の精度向上はRAGシステム全体の性能を左右します。以下では、最新の研究・実
例を参考に採用する検索戦略をまとめます。

1. **ベクトル検索による類似度探索**: 質問文を埋め込みベクトルに変換し、コサイン
   類似度等を用いて近傍チャンクを取得します。言い回しが異なっていても意味が近け
   ればヒットします。
2. **キーワードとのハイブリッド検索**: 特殊なフレーズを確実に拾うため、BM25など
   のキーワード検索結果をベクトル検索結果とマージします。ベクトルDB側のハイブリ
   ッドクエリ機能も検討します。
3. **検索結果のリランキング**: 初期検索で得た候補チャンクを小規模モデルで再ラン
   ク付けし、関連度の高いものからLLMへ渡します。
4. **クエリ再構成**: 複合質問など必要に応じてサブクエリ分割やリフレーズを行い、
   検索漏れを防ぎます。
5. **スケーラビリティとキャッシュ**: 大量データでも応答速度を維持できるよう、近
   似近傍探索アルゴリズムやクエリキャッシュを利用します。アクセス制御メタデータ
   を検索フィルタにも活用します。

これらの戦略により、ユーザーが求める情報を高い再現率と精度で抽出することを目指し
ます。
