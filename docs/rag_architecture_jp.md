# RAGシステムのアーキテクチャ

以下では社内ドキュメントQAシステムにおけるRetrieval-Augmented Generation (RAG) の基本的な処理フローと必要要件を整理します。

## 全体アーキテクチャ
図1はオフラインのインデックス構築フェーズとオンラインのクエリ応答フェーズから成るRAGパイプラインを示します。

- **インデックス構築フェーズ**: SharePoint から文書を収集し、テキスト変換・前処理後にチャンク分割を行います。各チャンクは埋め込みベクトルを計算してベクターデータベースへ保存します。
- **クエリ応答フェーズ**: ユーザー質問をベクトル化し、関連するチャンクを検索してLLMに文脈として与えます。LLMの生成結果に出典情報付与や整形処理を行い回答を返します。

### 文書インデックス構築
1. **SharePoint 取得**: Microsoft Graph API などでファイルをダウンロードし、増分クロールを行います。
2. **テキスト抽出**: Word、PowerPoint、Excel、PDF それぞれに適した方法でテキストを取得します。必要に応じOCRを使います。
3. **メタデータ付与**: タイトルや更新日時、アクセス権情報を記録します。
4. **前処理**: 不要な空白や改ページを除去し、見出し構造を保持したまま整形します。
5. **チャンク分割**: 見出しや段落単位を考慮して200〜300語程度でセマンティックチャンク化し、重なりも設けます。
6. **ベクトル変換**: 適切な多言語 / 日本語向け埋め込みモデルでベクトル化します。
7. **ベクターデータベース格納**: チャンクのベクトルとメタデータをChromaやFaiss等に保存します。必要に応じ全文検索エンジンにも登録します。
8. **更新監視**: 変更ログやWebhookで文書更新を検知し、再インデックスを行います。ストリーミング処理での即時アップサートも検討します。

## クエリ応答フロー
1. チャットUIから受け取った質問をベクトル化し、ベクターストアから類似チャンクを取得します。
2. 見つかったチャンクを文脈としてLLMに入力し、社内情報に基づく回答を生成します。
3. 回答には引用元などの出典情報を付与し、必要に応じてフォーマットを整えユーザーへ返します。

このアーキテクチャにより、社内の最新情報を活用したリアルタイムな質疑応答が可能になります。

## 高精度な情報検索戦略
検索段階の精度向上はRAGシステム全体の性能を左右します。ここでは最新研究や実運用
例で効果が確認されている手法を盛り込みます。

1. **ベクトル検索 (Semantic Search)**: 質問文を埋め込みベクトルに変換し、コサ
   イン類似度で近傍ベクトルを取得します。言い回しが異なっても意味が近ければ検
   索可能です。
2. **ハイブリッド検索 (ベクトル＋キーワード)**: ベクトル検索だけでは漏れるケー
   スを補うため、BM25 などキーワード検索も併用します。両者の結果をマージするこ
   とでリコールを高めます。
3. **検索結果のリランキング**: 初期検索で得た候補チャンクをクロスエンコーダ型
   の小規模モデルで再ランク付けし、最終的にLLMへ渡す上位数件を決定します。
4. **クエリの高度な処理**: 必要に応じて質問をサブクエリへ分割したり、初回結果か
   ら追加キーワードを抽出して再検索する疑似関連フィードバックを行います。
5. **検索の効率とスケーラビリティ**: 近似近傍探索アルゴリズム(HNSW等)の活用やク
   エリキャッシュにより、数百万規模のチャンクを扱う場合でも高速応答を維持します。
6. **アクセス制御によるフィルタ**: メタデータの機密区分や所属部署を基に、ユーザ
   ー権限に合致した文書のみを検索対象とします。

これらの戦略を組み合わせることで、ユーザーが求める情報を高い再現率と精度で抽出す
ることを目指します。
