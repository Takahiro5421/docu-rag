# RAGシステムのアーキテクチャ

以下では社内ドキュメントQAシステムにおけるRetrieval-Augmented Generation (RAG) の基本的な処理フローと必要要件を整理します。

## 全体アーキテクチャ
図1はオフラインのインデックス構築フェーズとオンラインのクエリ応答フェーズから成るRAGパイプラインを示します。

- **インデックス構築フェーズ**: SharePoint から文書を収集し、テキスト変換・前処理後にチャンク分割を行います。各チャンクは埋め込みベクトルを計算してベクターデータベースへ保存します。
- **クエリ応答フェーズ**: ユーザー質問をベクトル化し、関連するチャンクを検索してLLMに文脈として与えます。LLMの生成結果に出典情報付与や整形処理を行い回答を返します。

### 文書インデックス構築
1. **SharePoint 取得**: Microsoft Graph API などでファイルをダウンロードし、増分クロールを行います。
2. **テキスト抽出**: Word、PowerPoint、Excel、PDF それぞれに適した方法でテキストを取得します。必要に応じOCRを使います。
3. **メタデータ付与**: タイトルや更新日時、アクセス権情報を記録します。
4. **前処理**: 不要な空白や改ページを除去し、見出し構造を保持したまま整形します。
5. **チャンク分割**: 見出しや段落単位を考慮して200〜300語程度でセマンティックチャンク化し、重なりも設けます。
6. **ベクトル変換**: 適切な多言語 / 日本語向け埋め込みモデルでベクトル化します。
7. **ベクターデータベース格納**: チャンクのベクトルとメタデータをChromaやFaiss等に保存します。必要に応じ全文検索エンジンにも登録します。
8. **更新監視**: 変更ログやWebhookで文書更新を検知し、再インデックスを行います。ストリーミング処理での即時アップサートも検討します。

## クエリ応答フロー
1. チャットUIから受け取った質問をベクトル化し、ベクターストアから類似チャンクを取得します。
2. 見つかったチャンクを文脈としてLLMに入力し、社内情報に基づく回答を生成します。
3. 回答には引用元などの出典情報を付与し、必要に応じてフォーマットを整えユーザーへ返します。

このアーキテクチャにより、社内の最新情報を活用したリアルタイムな質疑応答が可能になります。
